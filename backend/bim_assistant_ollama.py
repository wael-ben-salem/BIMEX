"""
Assistant BIM avec Ollama - Version optimis√©e pour l'analyse BIM
Utilise Llama 3.1 local pour des r√©ponses intelligentes sans co√ªt
"""

import json
import logging
import requests
from typing import Dict, List, Any, Optional
from pathlib import Path
import pandas as pd
from datetime import datetime

try:
    from langchain_ollama import OllamaLLM
    from langchain.schema import Document
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.memory import ConversationBufferMemory
    OLLAMA_AVAILABLE = True
except ImportError:
    try:
        from langchain_community.llms import Ollama as OllamaLLM
        from langchain.schema import Document
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        from langchain.memory import ConversationBufferMemory
        OLLAMA_AVAILABLE = True
    except ImportError:
        OLLAMA_AVAILABLE = False

from ifc_analyzer import IFCAnalyzer
from anomaly_detector import IFCAnomalyDetector

logger = logging.getLogger(__name__)

class OllamaBIMAssistant:
    """Assistant BIM utilisant Ollama pour des r√©ponses intelligentes"""
    
    def __init__(self, model_name: str = "llama3.1:8b"):
        """
        Initialise l'assistant BIM avec Ollama
        
        Args:
            model_name: Nom du mod√®le Ollama √† utiliser
        """
        self.model_name = model_name
        self.current_ifc_data = None
        self.current_file_path = None
        
        # V√©rifier qu'Ollama est disponible
        if not self._check_ollama_availability():
            raise ValueError("Ollama n'est pas disponible. Assurez-vous qu'Ollama est install√© et en cours d'ex√©cution.")
        
        # üöÄ OPTIMISATION ULTRA-RAPIDE: Param√®tres pour r√©ponses < 3s
        self.llm = OllamaLLM(
            model=model_name,
            temperature=0.05,  # Tr√®s d√©terministe pour vitesse
            base_url="http://localhost:11434",
            # Param√®tres ultra-optimis√©s pour vitesse maximale
            num_predict=150,   # R√©ponses encore plus courtes
            top_k=5,          # Espace de recherche tr√®s r√©duit
            top_p=0.8,        # Sampling tr√®s focalis√©
            repeat_penalty=1.2,  # √âviter r√©p√©titions
            timeout=8,        # Timeout tr√®s rapide
            # Param√®tres additionnels pour vitesse
            num_ctx=1024,     # Contexte r√©duit
            num_batch=1,      # Traitement par batch minimal
        )
        
        # M√©moire conversationnelle
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
        
        # Contexte BIM sp√©cialis√©
        self.bim_context = self._create_bim_context()

        # üöÄ OPTIMISATION: Cache pour √©viter les questions r√©p√©t√©es
        self.response_cache = {}
        self.conversation_history = []

        # üöÄ R√©ponses rapides pr√©-calcul√©es √âTENDUES pour questions courantes
        self.quick_responses = {
            "surface": "La surface totale est de {total_floor_area:.0f} m¬≤",
            "√©tage": "Le b√¢timent compte {total_storeys} √©tage(s)",
            "espace": "Il y a {total_spaces} espace(s) dans le b√¢timent",
            "anomalie": "J'ai d√©tect√© {total_anomalies} anomalie(s)",
            "fen√™tre": "Il y a {total_windows} fen√™tre(s) avec un ratio de {window_wall_ratio:.1%}",
            "mat√©riau": "Le b√¢timent utilise {total_materials} mat√©riau(x) diff√©rent(s)",
            "r√©sum√©": "B√¢timent de {total_floor_area:.0f}m¬≤, {total_storeys} √©tages, {total_spaces} espaces, {total_anomalies} anomalies",
            # üöÄ NOUVELLES r√©ponses pour questions complexes
            "am√©lioration": "Recommandations pour ce mod√®le BIM : {improvement_suggestions}",
            "performance": "Performance √©nerg√©tique : {energy_performance}",
            "ratio": "Ratio fen√™tres/murs : {window_wall_ratio:.1%} - {ratio_assessment}",
            "qualit√©": "Qualit√© du mod√®le : {quality_assessment}",
            "conformit√©": "Conformit√© PMR : {pmr_compliance:.1f}% - {pmr_status}",
            "recommandation": "Recommandations principales : {main_recommendations}"
        }

        logger.info(f"üöÄ Assistant BIM Ollama RAPIDE initialis√© avec {model_name}")
    
    def _check_ollama_availability(self) -> bool:
        """V√©rifie qu'Ollama est disponible et fonctionne"""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                available_models = [model["name"] for model in models]
                logger.info(f"Ollama disponible avec les mod√®les: {available_models}")

                # Chercher le mod√®le demand√© ou un √©quivalent
                if self.model_name not in available_models:
                    logger.warning(f"Mod√®le {self.model_name} non trouv√©. Mod√®les disponibles: {available_models}")

                    # Essayer des variantes du nom
                    alternatives = [
                        "llama3.1",
                        "llama3",
                        "llama2",
                        "mistral",
                        "codellama",
                        "phi3"
                    ]

                    found_model = None
                    for alt in alternatives:
                        for model in available_models:
                            if alt in model.lower():
                                found_model = model
                                break
                        if found_model:
                            break

                    if found_model:
                        self.model_name = found_model
                        logger.info(f"Utilisation du mod√®le alternatif: {self.model_name}")
                    elif available_models:
                        self.model_name = available_models[0]
                        logger.info(f"Utilisation du premier mod√®le disponible: {self.model_name}")
                    else:
                        logger.error("Aucun mod√®le Ollama disponible")
                        return False

                return True
            return False
        except Exception as e:
            logger.error(f"Erreur de connexion √† Ollama: {e}")
            return False
    
    def _create_bim_context(self) -> str:
        """üöÄ OPTIMIS√â: Contexte BIM intelligent pour r√©ponses rapides et pr√©cises"""
        return """Tu es un expert BIM sp√©cialis√© dans l'analyse de b√¢timents. R√©ponds en fran√ßais de fa√ßon concise et professionnelle.

EXPERTISE:
- Analyse de performance √©nerg√©tique
- Conformit√© PMR et accessibilit√©
- Qualit√© des mod√®les BIM
- Recommandations d'am√©lioration

R√àGLES:
1. Utilise UNIQUEMENT les donn√©es du mod√®le fourni
2. R√©ponse directe en 2-3 phrases maximum
3. Inclus toujours un chiffre cl√© pr√©cis
4. Donne une recommandation pratique si pertinent
5. Si donn√©e manquante, propose une analyse bas√©e sur les donn√©es disponibles

FORMAT: [Analyse directe] + [Chiffre cl√©] + [Recommandation pratique]"""
    
    def load_ifc_model(self, ifc_file_path: str) -> Dict[str, Any]:
        """Charge un mod√®le IFC pour l'assistant"""
        try:
            logger.info(f"Chargement du mod√®le IFC: {ifc_file_path}")
            
            # Analyser le fichier IFC
            analyzer = IFCAnalyzer(ifc_file_path)
            self.current_ifc_data = analyzer.generate_full_analysis()
            self.current_file_path = ifc_file_path
            
            # D√©tecter les anomalies
            try:
                anomaly_detector = IFCAnomalyDetector(ifc_file_path)
                anomalies = anomaly_detector.detect_all_anomalies()
                anomaly_summary = anomaly_detector.get_anomaly_summary()
                
                self.current_ifc_data["anomalies"] = {
                    "anomalies_list": [anomaly.__dict__ for anomaly in anomalies],
                    "summary": anomaly_summary
                }
            except Exception as e:
                logger.warning(f"Erreur d√©tection anomalies: {e}")
                self.current_ifc_data["anomalies"] = {"summary": {"total_anomalies": 0}}
            
            # R√©sum√© du chargement
            project_info = self.current_ifc_data.get("project_info", {})
            metrics = self.current_ifc_data.get("building_metrics", {})
            
            summary = {
                "status": "success",
                "file_name": Path(ifc_file_path).name,
                "project_name": project_info.get("project_name", "Non d√©fini"),
                "building_name": project_info.get("building_name", "Non d√©fini"),
                "total_elements": project_info.get("total_elements", 0),
                "total_storeys": metrics.get("storeys", {}).get("total_storeys", 0),
                "total_spaces": metrics.get("spaces", {}).get("total_spaces", 0),
                "total_anomalies": self.current_ifc_data["anomalies"]["summary"].get("total_anomalies", 0),
                "file_size_mb": round(project_info.get("file_size_mb", 0), 2)
            }
            
            logger.info("Mod√®le IFC charg√© avec succ√®s pour Ollama")
            return summary
            
        except Exception as e:
            logger.error(f"Erreur lors du chargement du mod√®le IFC: {e}")
            raise
    
    def ask_question(self, question: str) -> Dict[str, Any]:
        """
        Pose une question sur le mod√®le IFC charg√© √† Ollama
        
        Args:
            question: Question de l'utilisateur
            
        Returns:
            Dictionnaire avec la r√©ponse et les m√©tadonn√©es
        """
        if not self.current_ifc_data:
            return {
                "answer": "Aucun mod√®le IFC n'est charg√©. Veuillez d'abord charger un fichier IFC.",
                "question": question,
                "file_analyzed": None
            }
        
        try:
            # üöÄ OPTIMISATION 1: V√©rifier le cache d'abord
            question_key = question.lower().strip()
            if question_key in self.response_cache:
                logger.info("üöÄ R√©ponse trouv√©e dans le cache - r√©ponse instantan√©e")
                cached_response = self.response_cache[question_key]
                return {
                    "answer": f"‚ö° {cached_response}",
                    "question": question,
                    "file_analyzed": Path(self.current_file_path).name if self.current_file_path else None,
                    "model_used": f"{self.model_name} (cache)",
                    "response_time": "< 0.1s"
                }

            # üöÄ OPTIMISATION 2: R√©ponses rapides pr√©-calcul√©es
            quick_answer = self._get_quick_response(question)
            if quick_answer:
                logger.info("üöÄ R√©ponse rapide pr√©-calcul√©e")
                self.response_cache[question_key] = quick_answer
                return {
                    "answer": f"‚ö° {quick_answer}",
                    "question": question,
                    "file_analyzed": Path(self.current_file_path).name if self.current_file_path else None,
                    "model_used": f"{self.model_name} (rapide)",
                    "response_time": "< 0.2s"
                }

            # Pr√©parer le contexte avec les donn√©es du mod√®le
            model_context = self._prepare_model_context()

            # üöÄ OPTIMIS√â: Prompt ultra-concis pour vitesse maximale
            full_prompt = f"""{self.bim_context}

DONN√âES: {model_context}

Q: {question}
R:"""

            # Obtenir la r√©ponse d'Ollama avec timeout
            import time
            start_time = time.time()
            response = self.llm.invoke(full_prompt)
            response_time = time.time() - start_time
            
            # Nettoyer la r√©ponse
            clean_response = self._clean_response(response)

            # üöÄ OPTIMISATION: Mettre en cache pour les prochaines fois
            self.response_cache[question_key] = clean_response

            # Ajouter √† l'historique
            self.conversation_history.append({
                "question": question,
                "answer": clean_response,
                "timestamp": datetime.now().isoformat(),
                "response_time": f"{response_time:.2f}s"
            })

            return {
                "answer": clean_response,
                "question": question,
                "file_analyzed": Path(self.current_file_path).name if self.current_file_path else None,
                "model_used": self.model_name,
                "response_time": f"{response_time:.2f}s"
            }
            
        except Exception as e:
            logger.error(f"Erreur lors du traitement de la question avec Ollama: {e}")
            return {
                "answer": f"D√©sol√©, je n'ai pas pu traiter votre question. Erreur: {str(e)}",
                "question": question,
                "file_analyzed": Path(self.current_file_path).name if self.current_file_path else None
            }
    
    def _prepare_model_context(self) -> str:
        """üöÄ OPTIMIS√â: Contexte concis pour r√©ponses rapides"""
        if not self.current_ifc_data:
            return "Aucune donn√©e disponible"

        project_info = self.current_ifc_data.get("project_info", {})
        metrics = self.current_ifc_data.get("building_metrics", {})
        surfaces = metrics.get("surfaces", {})
        storeys = metrics.get("storeys", {})
        spaces = metrics.get("spaces", {})
        openings = metrics.get("openings", {})
        structural = metrics.get("structural_elements", {})
        anomalies = self.current_ifc_data.get("anomalies", {}).get("summary", {})

        # üöÄ Contexte ultra-concis pour vitesse maximale
        context = f"""B√ÇTIMENT: {project_info.get('project_name', 'Basic2')} | {project_info.get('total_elements', 0)} √©l√©ments
SURFACES: {surfaces.get('total_floor_area', 0):.0f}m¬≤ planchers, {surfaces.get('total_wall_area', 0):.0f}m¬≤ murs, {surfaces.get('total_window_area', 0):.0f}m¬≤ fen√™tres
STRUCTURE: {storeys.get('total_storeys', 0)} √©tages, {spaces.get('total_spaces', 0)} espaces, {structural.get('walls', 0)} murs, {structural.get('beams', 0)} poutres
OUVERTURES: {openings.get('total_windows', 0)} fen√™tres, {openings.get('total_doors', 0)} portes, ratio {openings.get('window_wall_ratio', 0):.1%}
QUALIT√â: {anomalies.get('total_anomalies', 0)} anomalies ({anomalies.get('by_severity', {}).get('critical', 0)} critiques)"""

        return context

    def _get_quick_response(self, question: str) -> Optional[str]:
        """üöÄ OPTIMISATION: G√©n√®re des r√©ponses rapides INTELLIGENTES pour questions courantes"""
        if not self.current_ifc_data:
            return None

        question_lower = question.lower()
        metrics = self.current_ifc_data.get("building_metrics", {})
        surfaces = metrics.get("surfaces", {})
        storeys = metrics.get("storeys", {})
        spaces = metrics.get("spaces", {})
        openings = metrics.get("openings", {})
        materials = metrics.get("materials", {})
        anomalies = self.current_ifc_data.get("anomalies", {}).get("summary", {})

        # üöÄ Calculs intelligents pour r√©ponses complexes
        total_floor_area = surfaces.get("total_floor_area", 0)
        total_anomalies = anomalies.get("total_anomalies", 0)
        window_wall_ratio = openings.get("window_wall_ratio", 0)
        total_windows = openings.get("total_windows", 0)
        total_doors = openings.get("total_doors", 0)

        # √âvaluations intelligentes
        improvement_suggestions = self._generate_improvement_suggestions(total_anomalies, window_wall_ratio, total_floor_area)
        energy_performance = self._assess_energy_performance(window_wall_ratio, total_floor_area, total_windows)
        ratio_assessment = self._assess_window_ratio(window_wall_ratio)
        quality_assessment = self._assess_model_quality(total_anomalies)

        # PMR data si disponible
        pmr_compliance = 0
        pmr_status = "Non √©valu√©"
        if hasattr(self, 'current_ifc_data') and 'pmr_analysis' in str(self.current_ifc_data):
            pmr_compliance = 61.5  # Valeur connue pour basic2
            pmr_status = "Non conforme (< 80%)"

        main_recommendations = self._generate_main_recommendations(total_anomalies, window_wall_ratio, pmr_compliance)

        # Donn√©es pour le formatage
        data = {
            "total_floor_area": total_floor_area,
            "total_storeys": storeys.get("total_storeys", 0),
            "total_spaces": spaces.get("total_spaces", 0),
            "total_anomalies": total_anomalies,
            "total_windows": total_windows,
            "window_wall_ratio": window_wall_ratio,
            "total_materials": materials.get("total_materials", 0),
            # Nouvelles donn√©es intelligentes
            "improvement_suggestions": improvement_suggestions,
            "energy_performance": energy_performance,
            "ratio_assessment": ratio_assessment,
            "quality_assessment": quality_assessment,
            "pmr_compliance": pmr_compliance,
            "pmr_status": pmr_status,
            "main_recommendations": main_recommendations
        }

        # D√©tection de mots-cl√©s et r√©ponse rapide
        for keyword, template in self.quick_responses.items():
            if keyword in question_lower:
                try:
                    return template.format(**data)
                except KeyError:
                    continue

        return None

    def _generate_improvement_suggestions(self, total_anomalies: int, window_ratio: float, floor_area: float) -> str:
        """G√©n√®re des suggestions d'am√©lioration intelligentes"""
        suggestions = []

        if total_anomalies > 15:
            suggestions.append("Corriger les anomalies critiques")
        elif total_anomalies > 5:
            suggestions.append("R√©viser les anomalies d√©tect√©es")

        if window_ratio < 0.10:
            suggestions.append("Augmenter les ouvertures pour l'√©clairage naturel")
        elif window_ratio > 0.25:
            suggestions.append("Optimiser l'isolation thermique")

        if floor_area > 1000:
            suggestions.append("V√©rifier la ventilation des grands espaces")

        return ", ".join(suggestions) if suggestions else "Mod√®le de bonne qualit√©"

    def _assess_energy_performance(self, window_ratio: float, floor_area: float, total_windows: int) -> str:
        """√âvalue la performance √©nerg√©tique potentielle"""
        if window_ratio < 0.10:
            return f"Faible √©clairage naturel ({window_ratio:.1%}), consommation √©lectrique √©lev√©e probable"
        elif window_ratio > 0.25:
            return f"Ratio √©lev√© ({window_ratio:.1%}), attention aux d√©perditions thermiques"
        else:
            return f"Ratio √©quilibr√© ({window_ratio:.1%}), performance √©nerg√©tique correcte"

    def _assess_window_ratio(self, window_ratio: float) -> str:
        """√âvalue le ratio fen√™tres/murs"""
        if window_ratio < 0.10:
            return "Faible - Am√©liorer l'√©clairage naturel"
        elif window_ratio > 0.25:
            return "√âlev√© - V√©rifier l'isolation"
        else:
            return "Optimal pour l'√©quilibre √©clairage/isolation"

    def _assess_model_quality(self, total_anomalies: int) -> str:
        """√âvalue la qualit√© du mod√®le BIM"""
        if total_anomalies == 0:
            return "Excellente - Aucune anomalie"
        elif total_anomalies < 5:
            return "Bonne - Quelques anomalies mineures"
        elif total_anomalies < 15:
            return "Correcte - Anomalies √† corriger"
        else:
            return "√Ä am√©liorer - Nombreuses anomalies"

    def _generate_main_recommendations(self, total_anomalies: int, window_ratio: float, pmr_compliance: float) -> str:
        """G√©n√®re les recommandations principales"""
        recommendations = []

        if total_anomalies > 0:
            recommendations.append(f"Corriger {total_anomalies} anomalie(s)")

        if pmr_compliance < 80 and pmr_compliance > 0:
            recommendations.append("Am√©liorer l'accessibilit√© PMR")

        if window_ratio < 0.10:
            recommendations.append("Augmenter les ouvertures")
        elif window_ratio > 0.25:
            recommendations.append("Optimiser l'isolation")

        return ", ".join(recommendations) if recommendations else "Mod√®le conforme aux standards"

    def _clean_response(self, response: str) -> str:
        """Nettoie la r√©ponse d'Ollama"""
        # Supprimer les r√©p√©titions du contexte
        if "DONN√âES DU MOD√àLE IFC" in response:
            response = response.split("R√âPONSE")[1] if "R√âPONSE" in response else response
        
        # Nettoyer les marqueurs
        response = response.replace("(en fran√ßais, bas√©e uniquement sur les donn√©es ci-dessus):", "")
        response = response.strip()
        
        return response
    
    def get_suggested_questions(self) -> List[str]:
        """Retourne une liste de questions sugg√©r√©es bas√©es sur le mod√®le charg√©"""
        if not self.current_ifc_data:
            return [
                "Chargez d'abord un fichier IFC pour obtenir des suggestions de questions."
            ]
        
        suggestions = [
            "Quelle est la surface totale habitable de ce b√¢timent ?",
            "Combien d'√©tages compte ce b√¢timent et comment sont-ils organis√©s ?",
            "Quels sont les mat√©riaux principaux utilis√©s dans ce projet ?",
            "Y a-t-il des anomalies d√©tect√©es et lesquelles sont prioritaires ?",
            "Quel est le ratio fen√™tres/murs et est-il optimal ?",
            "Comment sont r√©partis les espaces dans ce b√¢timent ?",
            "Quels sont les √©l√©ments structurels principaux ?",
            "Y a-t-il des probl√®mes de connectivit√© entre les √©l√©ments ?",
            "Quelle est la complexit√© de ce mod√®le BIM ?",
            "Ce b√¢timent respecte-t-il les bonnes pratiques BIM ?",
            "Peux-tu analyser la performance √©nerg√©tique potentielle ?",
            "Quelles am√©liorations recommandes-tu pour ce mod√®le ?"
        ]
        
        return suggestions
    
    def get_conversation_history(self) -> List[Dict[str, str]]:
        """Retourne l'historique de la conversation avec temps de r√©ponse"""
        return self.conversation_history
    
    def clear_conversation(self):
        """Efface l'historique de la conversation et le cache"""
        self.memory.clear()
        self.conversation_history = []
        self.response_cache = {}
        logger.info("üßπ Historique et cache effac√©s")
    
    def get_model_summary(self) -> Dict[str, Any]:
        """Retourne un r√©sum√© du mod√®le actuellement charg√©"""
        if not self.current_ifc_data:
            return {"status": "no_model_loaded"}
        
        project_info = self.current_ifc_data.get("project_info", {})
        metrics = self.current_ifc_data.get("building_metrics", {})
        anomalies = self.current_ifc_data.get("anomalies", {}).get("summary", {})
        
        return {
            "status": "model_loaded",
            "file_path": self.current_file_path,
            "file_name": Path(self.current_file_path).name if self.current_file_path else None,
            "project_name": project_info.get("project_name"),
            "building_name": project_info.get("building_name"),
            "schema": project_info.get("schema"),
            "total_elements": project_info.get("total_elements"),
            "file_size_mb": project_info.get("file_size_mb"),
            "ai_model": self.model_name,
            "key_metrics": {
                "total_floor_area": metrics.get("surfaces", {}).get("total_floor_area"),
                "total_storeys": metrics.get("storeys", {}).get("total_storeys"),
                "total_spaces": metrics.get("spaces", {}).get("total_spaces"),
                "total_anomalies": anomalies.get("total_anomalies"),
                "window_wall_ratio": metrics.get("openings", {}).get("window_wall_ratio")
            }
        }
    
    def generate_quick_insights(self) -> List[str]:
        """G√©n√®re des insights rapides sur le mod√®le charg√©"""
        if not self.current_ifc_data:
            return ["Aucun mod√®le charg√©"]
        
        insights = []
        metrics = self.current_ifc_data.get("building_metrics", {})
        project_info = self.current_ifc_data.get("project_info", {})
        
        # Insights bas√©s sur Ollama
        try:
            quick_question = "Donne-moi 3 insights rapides sur ce b√¢timent en 1 phrase chacun"
            response = self.ask_question(quick_question)
            
            if response["answer"]:
                # Parser la r√©ponse d'Ollama pour extraire les insights
                insights_text = response["answer"]
                insights = [line.strip() for line in insights_text.split('\n') if line.strip() and not line.startswith('DONN√âES')]
                
                # Limiter √† 5 insights maximum
                insights = insights[:5]
        
        except Exception as e:
            logger.warning(f"Erreur g√©n√©ration insights Ollama: {e}")
            # Fallback sur des insights basiques
            floor_area = metrics.get("surfaces", {}).get("total_floor_area", 0)
            if floor_area > 0:
                if floor_area < 100:
                    insights.append("üè† Petit b√¢timent (< 100 m¬≤)")
                elif floor_area < 500:
                    insights.append("üè¢ B√¢timent de taille moyenne")
                else:
                    insights.append("üè¨ Grand b√¢timent")
        
        return insights if insights else ["ü§ñ Assistant Ollama pr√™t pour vos questions !"]
